<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
    
  
  <link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






  

<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="NLP," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="we&#39;re gonna talk about ELMO &amp; GPT &amp; Bert.">
<meta property="og:type" content="article">
<meta property="og:title" content="Note of CS224n —— ELMO &amp; GPT &amp; Bert">
<meta property="og:url" content="http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html">
<meta property="og:site_name" content="Near&#39;s Notes">
<meta property="og:description" content="we&#39;re gonna talk about ELMO &amp; GPT &amp; Bert.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://up.enterdesk.com/edpic/53/da/9b/53da9bfa41bc04ff808e8cd96fb933a8.jpg">
<meta property="article:published_time" content="2020-07-03T13:54:15.000Z">
<meta property="article:modified_time" content="2020-07-03T15:56:15.667Z">
<meta property="article:author" content="Near zeng">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://up.enterdesk.com/edpic/53/da/9b/53da9bfa41bc04ff808e8cd96fb933a8.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html"/>





<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>


  <title> Note of CS224n —— ELMO & GPT & Bert | Near's Notes </title>
<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?340874ba9357cbe81570aa4ac1185941";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta custom-logo">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Near's Notes</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <h1 class="site-subtitle" itemprop="description">NN</h1>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Near zeng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Near's Notes">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Near's Notes" src="/images/avatar.jpg">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                Note of CS224n —— ELMO & GPT & Bert
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-07-03T21:54:15+08:00">
                2020-07-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%A5%E9%97%A8/" itemprop="url" rel="index">
                    <span itemprop="name">入门</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%A5%E9%97%A8/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
		  
			 
          
          
		   
          

		  
            <div class="post-wordcount">
              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  943
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          
		  
          
              <div class="post-description">
                  we're gonna talk about ELMO & GPT & Bert.
              </div>
          
 
        


        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      
        <div class="post-gallery" itemscope itemtype="http://schema.org/ImageGallery">
          
          
            <div class="post-gallery-row">
              <a class="post-gallery-img fancybox"
                 href="https://up.enterdesk.com/edpic/53/da/9b/53da9bfa41bc04ff808e8cd96fb933a8.jpg" rel="gallery_ckc6ek41p000psayn2o5k4rcw"
                 itemscope itemtype="http://schema.org/ImageObject" itemprop="url">
                <img src="https://up.enterdesk.com/edpic/53/da/9b/53da9bfa41bc04ff808e8cd96fb933a8.jpg" itemprop="contentUrl"/>
              </a>
            
          

          
          </div>
        </div>
      

      
        <h2 id="1-ELMo"><a href="#1-ELMo" class="headerlink" title="1. ELMo"></a>1. ELMo</h2><p>In the previous work of word2vec and GloVe, each word corresponds to a vector, which is powerless for polysemous words. And as the language environment changes, these vectors cannot accurately express the corresponding features. The authors of ELMo believe that a good word representation model should take into account two issues at the same time: one is the complex semantic and grammatical characteristics of word usage; and the other is that these usages should change with the change of language environment.</p>
<p><strong>The characteristics of ELMo</strong> : the representation of each word is a function of the entire input sentence. </p>
<h3 id="1-1-How-ELMo-works"><a href="#1-1-How-ELMo-works" class="headerlink" title="1.1 How ELMo works"></a>1.1 How ELMo works</h3><ol>
<li>Pre-train biLM models on large corpora. The model is composed of two layers of bi-LSTM, and the models are connected by residual connection. Moreover, the author believes that the low-level bi-LSTM layer can extract the syntactic information in the corpus, and the high-level bi-LSTM can extract the semantic information in the corpus.</li>
<li>In our training corpus (removing labels), fine-tuning the pre-trained biLM model. This step can be seen as the domain transfer of biLM.</li>
<li>The word embedding generated by ELMo is used as the input of the task, and sometimes it can be added both during input and output.</li>
</ol>
<p>ELMo got its name (Embeddings from Language Models). In order to be used in downstream NLP tasks, generally use the corpus of downstream tasks (note that the label is omitted here) to fine-tune the language model. This fine-tuning is equivalent to a domain transfer; then use the label information for supervised learning.</p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/useELMo.png" alt></p>
<h3 id="1-2-Bidirectional-language-models"><a href="#1-2-Bidirectional-language-models" class="headerlink" title="1.2 Bidirectional language models"></a>1.2 Bidirectional language models</h3><p>ELMo, as the name implies, is embeddings from Language Models, to be precise from Bidirectional language models. It can be expressed as:</p>
<p><img src="https://pirctures.oss-cn-beijing.aliyuncs.com/img/1.png" alt></p>
<p><strong>Forward LSTM structure:</strong></p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/f1.png" alt></p>
<p><strong>Reverse LSTM structure:</strong></p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/f2.png" alt></p>
<p><strong>Maximum likelihood function:</strong></p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/f3.png" alt></p>
<h3 id="1-3-summary"><a href="#1-3-summary" class="headerlink" title="1.3 summary"></a>1.3 summary</h3><p><img src="/blog/2020/07/03/notes-of-CS224n-part4/great.png" alt></p>
<p>In general, ELMo provides dynamic representation at the word level, which can effectively capture contextual information and solve the problem of polysemy.</p>
<h2 id="2-GPT"><a href="#2-GPT" class="headerlink" title="2. GPT"></a>2. GPT</h2><h3 id="2-1-Introduce"><a href="#2-1-Introduce" class="headerlink" title="2.1 Introduce"></a>2.1 Introduce</h3><p>OpenAI proposed the GPT model in the paper “Improving Language Understanding by Generative Pre-Training”, and later proposed the GPT2 model in the paper “Language Models are Unsupervised Multitask Learners”. The model structure of GPT2 and GPT is not much different, but a larger data set is used for the experiment.</p>
<p>The training method adopted by GPT is divided into two steps. The first step is to train the language model using an unlabeled text data set. The second step is to fine-tune the model according to specific downstream tasks, such as QA, text classification, etc.</p>
<h3 id="2-2-Structure"><a href="#2-2-Structure" class="headerlink" title="2.2 Structure"></a>2.2 Structure</h3><p>GPT uses Transformer’s Decoder structure and makes some changes to Transformer Decoder. The original Decoder contains two Multi-Head Attention structures, and GPT only retains Mask Multi-Head Attention.</p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/p1.png" alt="GPT&#39;s Decoder"></p>
<p>The following figure is the overall model of GPT, which contains 12 Decoders:</p>
<p><img src="/blog/2020/07/03/notes-of-CS224n-part4/p2.png" alt></p>
<h3 id="2-3-Summary"><a href="#2-3-Summary" class="headerlink" title="2.3 Summary"></a>2.3 Summary</h3><p>The GPT pre-training uses the above to predict the next word. GPT is more suitable for the task of text generation, because text generation is usually based on the information currently available to generate the next word.</p>
<h2 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3. BERT"></a>3. BERT</h2><h3 id="3-1-Introduce"><a href="#3-1-Introduce" class="headerlink" title="3.1 Introduce"></a>3.1 Introduce</h3><p>BERT(Bidirectional Encoder Representation from Transformers) is the encoder of the bidirectional Transformer. The model architecture of BERT is based on multi-layer bidirectional conversion decoding. Because the decoder cannot obtain the information to be predicted, the main innovation of the model is in the pre-traing method, Which uses Masked LM and Next Sentence Prediction to capture word and sentence level representation respectively.</p>
<p>“Bidirectional” means that when the model is processing a certain word, it can use both the previous word and the following words. BERT is different from the traditional language model. It is not giving you all the previous words so that you can predict the most probable current word, but covering some words at random, and then use all the words that are not covered to make predictions.</p>
<h3 id="3-2-Structure"><a href="#3-2-Structure" class="headerlink" title="3.2 Structure"></a>3.2 Structure</h3><p><img src="/blog/2020/07/03/notes-of-CS224n-part4/BERT.png" alt></p>
<p>BERT provides a simple and a complex model, the corresponding hyperparameters are as follows:</p>
<p><strong>BERT-base</strong>: L=12, H=768, A=12, total parameter 110M;<br><strong>BERT-large</strong>: L=24, H=1024, A=16, the total amount of parameters is 340M;<br>In the above hyperparameters, L represents the number of layers of the network (the number of Transformer blocks), A represents the number of self-Attention in Multi-Head Attention, and the filter size is 4H.</p>
<h3 id="3-3-Pre-train-task"><a href="#3-3-Pre-train-task" class="headerlink" title="3.3 Pre-train task"></a>3.3 Pre-train task</h3><p>BERT is a multi-task model. Its task is composed of two self-supervised tasks, namely MLM and NSP.</p>
<h4 id="3-3-1-Task-1：-Masked-Language-Model"><a href="#3-3-1-Task-1：-Masked-Language-Model" class="headerlink" title="3.3.1 Task #1： Masked Language Model"></a>3.3.1 Task #1： Masked Language Model</h4><p>Masked Language Model (MLM) core idea is taken from a paper published by Wilson Taylor in 1953. It refers to masking some words from the input expectations during training, and then predicting the words through the context. This task is very similar to the cloze that we often do in middle school. Just like traditional language model algorithms and RNN matching, this property of MLM matches the structure of Transformer very well.</p>
<h4 id="3-3-2-Task-2-Next-Sentence-Prediction"><a href="#3-3-2-Task-2-Next-Sentence-Prediction" class="headerlink" title="3.3.2 Task #2: Next Sentence Prediction"></a>3.3.2 Task #2: Next Sentence Prediction</h4><p>The task of Next Sentence Prediction (NSP) is to determine whether sentence B is the following of sentence A. If yes, output <strong>“IsNext”</strong>, otherwise output <strong>“NotNext”</strong>. The training data is generated by randomly extracting two consecutive sentences from the parallel corpus, of which 50% retain the two extracted sentences, which are in accordance with the <strong>IsNext</strong> relationship, and the other 50% of the second sentence is randomly extracted from the expected, they The relationship is <strong>NotNext</strong>. </p>
<h2 id="4-References"><a href="#4-References" class="headerlink" title="4. References"></a>4. References</h2><h5 id="1-Deep-contextualized-word-representations"><a href="#1-Deep-contextualized-word-representations" class="headerlink" title="1. Deep contextualized word representations"></a>1. <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">Deep contextualized word representations</a></h5><h5 id="2-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#2-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="2. Improving Language Understanding by Generative Pre-Training"></a>2. <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></h5><h5 id="3-OpenAI-GPT-和-GPT2-模型详解"><a href="#3-OpenAI-GPT-和-GPT2-模型详解" class="headerlink" title="3. OpenAI GPT 和 GPT2 模型详解"></a>3. <a href="https://baijiahao.baidu.com/s?id=1652093322137148754" target="_blank" rel="noopener">OpenAI GPT 和 GPT2 模型详解</a></h5><h5 id="4-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#4-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>4. <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h5><h5 id="5-BERT详解"><a href="#5-BERT详解" class="headerlink" title="5. BERT详解"></a>5. <a href="https://zhuanlan.zhihu.com/p/48612853" target="_blank" rel="noopener">BERT详解</a></h5>
      
    </div>

    <div>
      
        

      
    </div>
  
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          
        </div>
      


    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>🐶 您的支持将鼓励我继续创作 🐶</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赞赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/images/wechat-reward-img.jpg" alt="Near zeng WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/images/alipay-reward-img.jpg" alt="Near zeng Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
        
     <div>    
      
      <ul class="post-copyright">
         <li class="post-copyright-link">
          <strong>本文作者：</strong>
          <a href="/" title="欢迎访问 Near zeng 的个人博客">Near zeng</a>
        </li>

        <li class="post-copyright-link">
          <strong>本文标题：</strong>
          <a href="http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html" title="Note of CS224n —— ELMO & GPT & Bert">Note of CS224n —— ELMO & GPT & Bert</a>
        </li>

        <li class="post-copyright-link">
          <strong>本文链接：</strong>
          <a href="http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html" title="Note of CS224n —— ELMO & GPT & Bert">http://www.7497.xyz/blog/2020/07/03/notes-of-CS224n-part4.html</a>
        </li>

        <li class="post-copyright-date">
            <strong>发布时间：</strong>2020年7月3日 - 21时07分
        </li>  

        <li class="post-copyright-license">
          <strong>版权声明： </strong>
          本文由 Near zeng 原创，采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="license" target="_blank">保留署名-非商业性使用-禁止演绎 4.0-国际许可协议</a> </br>转载请保留以上声明信息！
        </li>
      </ul>
    
  </div>  
      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2020/06/29/notes-of-CS224n-part3.html" rel="next" title="Note of CS224n —— subword">
                <i class="fa fa-chevron-left"></i> Note of CS224n —— subword
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMjI4My84ODQ3"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Near zeng" />
          <p class="site-author-name" itemprop="name">Near zeng</p>
          <p class="site-description motion-element" itemprop="description">a place for fun</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/nearzeng" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://weibo.com/7333272869/profile?topnav=1&wvr=6&is_all=1" target="_blank" title="weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/near-63-53" target="_blank" title="zhihu">
                  
                    <i class="fa fa-fw fa-battery-3"></i>
                  
                  zhihu
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.zhihu.com/people/near-63-53" title="友链出租" target="_blank">友链出租</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-ELMo"><span class="nav-number">1.</span> <span class="nav-text">1. ELMo</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-How-ELMo-works"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 How ELMo works</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Bidirectional-language-models"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Bidirectional language models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-summary"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-GPT"><span class="nav-number">2.</span> <span class="nav-text">2. GPT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Introduce"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Introduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Structure"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Summary"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-BERT"><span class="nav-number">3.</span> <span class="nav-text">3. BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Introduce"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Introduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Structure"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Pre-train-task"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Pre-train task</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-Task-1：-Masked-Language-Model"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 Task #1： Masked Language Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-Task-2-Next-Sentence-Prediction"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 Task #2: Next Sentence Prediction</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-References"><span class="nav-number">4.</span> <span class="nav-text">4. References</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Deep-contextualized-word-representations"><span class="nav-number">4.0.0.1.</span> <span class="nav-text">1. Deep contextualized word representations</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-Improving-Language-Understanding-by-Generative-Pre-Training"><span class="nav-number">4.0.0.2.</span> <span class="nav-text">2. Improving Language Understanding by Generative Pre-Training</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-OpenAI-GPT-和-GPT2-模型详解"><span class="nav-number">4.0.0.3.</span> <span class="nav-text">3. OpenAI GPT 和 GPT2 模型详解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><span class="nav-number">4.0.0.4.</span> <span class="nav-text">4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-BERT详解"><span class="nav-number">4.0.0.5.</span> <span class="nav-text">5. BERT详解</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Near zeng</span>
</div>
<!--

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io" target="_blank" rel="external nofollow noopener">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external nofollow noopener">
    NexT.Mist
  </a>| Hosted by <a href="https://pages.coding.me" target="_blank" rel="noopener" style="font-weight: bold">Coding Pages</a>
</div>

-->




        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  <script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script>

  
  <script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script>

  
  <script type="text/javascript" src="/vendors/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  

  
      <!-- UY BEGIN -->
      <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid="></script>
      <!-- UY END -->
  



  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  


  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url).substring(1);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

	<!-- 页面点击小红心 
<script type="text/javascript" src="/js/src/love.js"></script>
-->
</body>
</html>
