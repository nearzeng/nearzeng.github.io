<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>零基础入门CV之赛题理解</title>
    <url>//blog/2020/05/22/cv-task1.html</url>
    <content><![CDATA[<h2 id="CV初探"><a href="#CV初探" class="headerlink" title="CV初探"></a>CV初探</h2><h4 id="CV是什么"><a href="#CV是什么" class="headerlink" title="CV是什么"></a>CV是什么</h4><p>计算机视觉（Computational Vision），英文简写CV，是一门“赋予机器自然视觉能力”的学科，是使用计算机模仿人类视觉系统的科学，让计算机拥有类似人类 提取、处理、理解和分析图像以及图像序列的能力。它是一门包含领域很广的综合性学科。从现阶段的研究来看，计算机视觉试图建立一种人工系统，提出的越来越多的理论和技术主要目的是为了从图像或者多维数据中获取信息。</p>
<h4 id="CV可以解决什么问题"><a href="#CV可以解决什么问题" class="headerlink" title="CV可以解决什么问题"></a>CV可以解决什么问题</h4><p>好奇CV可以做什么，是一个比较适用的兴趣点和动力源。一般来说，CV的目标是”看到“。其在具体应用中往往都要解决一些相同的问题，这些问题包括：</p>
<ol>
<li><strong>识别</strong> —— ”看到“图像中的特定目标并能识别出来。</li>
<li><strong>运动</strong> —— ”看到“序列图像中物体的运动和自己在运动，或跟踪运动的物体。</li>
<li><strong>场景重建</strong> —— ”看到“序列图像并建立起完整的三维表面模型。</li>
<li><strong>图像恢复</strong> —— ”看到“一些被影响的部分背后的内容，主要是移除噪声和改善模糊状态</li>
</ol>
<h4 id="CV工程的常见步骤"><a href="#CV工程的常见步骤" class="headerlink" title="CV工程的常见步骤"></a>CV工程的常见步骤</h4><p>简单了解一下工程方面的步骤，有利于知道后续的实践和增加对CV的认识。一般来说，CV工程分为以下5步：</p>
<ol>
<li><strong>图像获取</strong> —— 通过各种图像感知器获取需要的图像数据。</li>
<li><strong>预处理</strong> —— 通过对图像进行一定的预处理来使其满足后续的操作要求。</li>
<li><strong>特征提取</strong> —— 从图像中提取各种复杂度的特征，以便识别目标。</li>
<li><strong>检测/分割</strong> —— 处理过程中，可能需要进一步分割有价值的部分用于后继处理。例如分割出包含特定目标的部分。</li>
<li><strong>高级处理</strong> —— 根据具体应用的需要，验证得到的数据是否符合要求，估测特定的系数等。</li>
</ol>
<h2 id="赛题理解"><a href="#赛题理解" class="headerlink" title="赛题理解"></a>赛题理解</h2><p>通过比赛来实践是一个学习的捷径，在压力之下可能会打破自己原来设定的界限，做到自我突破。当然，对我们这种新手来说，还是要脚踏实地一步一步来的。先来了解下Datawhale与天池联合发起的零基础入门系列赛事第二场 —— <a href="https://tianchi.aliyun.com/competition/entrance/531795/introduction" target="_blank" rel="noopener">零基础入门CV赛事之街景字符识别</a>。</p>
<ul>
<li><strong>赛题名称</strong>：零基础入门CV之街道字符识别            </li>
<li><strong>赛题目标</strong>：通过这道赛题可以引导大家走入计算机视觉的世界，主要针对竞赛选手上手视觉赛题，提高对数据建模能力。       </li>
<li><strong>赛题任务</strong>：赛题以计算机视觉中字符识别为背景，要求选手预测街道字符编码，这是一个典型的字符识别问题。<br>为了简化赛题难度，赛题数据采用公开数据集<a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">SVHN</a>，因此大家可以选择很多相应的paper作为思路参考。          <h4 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h4></li>
</ul>
<p>赛题以街道字符为赛题数据，数据集报名后可见并可下载，该数据来自公开数据集SVHN收集的街道字符，并进行了匿名采样处理。  </p>
<p><img src="/blog/2020/05/22/cv-task1/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%B7%E6%9C%AC%E5%B1%95%E7%A4%BA.png" alt="数据示例"></p>
<p><strong>注意: 按照比赛规则，所有的参赛选手只能使用比赛给定的数据集完成训练，不能使用SVHN原始数据集进行训练。比赛结束后将会对Top选手进行代码审核，违规的选手将清除排行榜成绩。</strong></p>
<p>训练集数据包括3W张照片，验证集数据包括1W张照片，每张照片包括颜色图像和对应的编码类别和具体位置；为了保证比赛的公平性，测试集A包括4W张照片，测试集B包括4W张照片。</p>
<p>需要注意的是本赛题需要选手识别图片中所有的字符，为了降低比赛难度，该赛题还提供了训练集、验证集和测试集中所有字符的位置框。</p>
<h4 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h4><p>对于训练数据每张图片将给出对应的编码标签和具体的字符框的位置（训练集、测试集和验证集都给出字符位置），可用于模型训练：</p>
<table>
<thead>
<tr>
<th align="center">Field</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center">top</td>
<td align="center">左上角坐标X</td>
</tr>
<tr>
<td align="center">height</td>
<td align="center">字符高度</td>
</tr>
<tr>
<td align="center">left</td>
<td align="center">左上角最表Y</td>
</tr>
<tr>
<td align="center">width</td>
<td align="center">字符宽度</td>
</tr>
<tr>
<td align="center">label</td>
<td align="center">字符编码</td>
</tr>
</tbody></table>
<p>字符的坐标具体如下所示：<img src="/blog/2020/05/22/cv-task1/%E5%AD%97%E7%AC%A6%E5%9D%90%E6%A0%87.png" alt="坐标">     </p>
<p> 在比赛数据（训练集、测试集和验证集）中，同一张图片中可能包括一个或者多个字符，因此在比赛数据的JSON标注中，会有多个字符的边框信息：</p>
<table>
<thead>
<tr>
<th align="center">原始图片</th>
<th align="center">图片JSON标注</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="/blog/2020/05/22/cv-task1/%E5%8E%9F%E5%A7%8B%E5%9B%BE%E7%89%87.png" alt="19"></td>
<td align="center"><img src="/blog/2020/05/22/cv-task1/%E5%8E%9F%E5%A7%8B%E5%9B%BE%E7%89%87%E6%A0%87%E6%B3%A8.png" alt="标注"></td>
</tr>
</tbody></table>
<h4 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h4><p> 选手提交结果与实际图片的编码进行对比，以编码整体识别准确率为评价指标。任何一个字符错误都为错误，最终评测指标结果越大越好，具体计算公式如下：<br>                                              Score = 编码识别正确的数量 / 测试集图片数量        </p>
<h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><p> 这里给出JSON中标签的读取方式：  </p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">import</span> json</span><br><span class="line">train_json = json.load(open(<span class="string">'../input/train.json'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标注处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_json</span><span class="params">(d)</span>:</span></span><br><span class="line">    arr = np.array([</span><br><span class="line">        d[<span class="string">'top'</span>], d[<span class="string">'height'</span>], d[<span class="string">'left'</span>],  d[<span class="string">'width'</span>], d[<span class="string">'label'</span>]</span><br><span class="line">    ])</span><br><span class="line">    arr = arr.astype(int)</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'../input/train/000000.png'</span>)</span><br><span class="line">arr = parse_json(train_json[<span class="string">'000000.png'</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, arr.shape[<span class="number">1</span>]+<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.xticks([]); plt.yticks([])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(arr.shape[<span class="number">1</span>]):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, arr.shape[<span class="number">1</span>]+<span class="number">1</span>, idx+<span class="number">2</span>)</span><br><span class="line">    plt.imshow(img[arr[<span class="number">0</span>, idx]:arr[<span class="number">0</span>, idx]+arr[<span class="number">1</span>, idx],arr[<span class="number">2</span>, idx]:arr[<span class="number">2</span>, idx]+arr[<span class="number">3</span>, idx]])</span><br><span class="line">    plt.title(arr[<span class="number">4</span>, idx])</span><br><span class="line">    plt.xticks([]); plt.yticks([])</span><br></pre></td></tr></table></figure>
<p><img src="/blog/2020/05/22/cv-task1/19.png" alt="19">     </p>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>赛题思路分析：<strong>赛题本质是一个分类识别问题，需要对图片的字符进行识别</strong>。但赛题给定的数据图片中不同图片中包含的字符数量不等。如下图所示，有的图片的字符个数为2，有的图片字符个数为3，有的图片字符个数为4。      </p>
<table>
<thead>
<tr>
<th>字符属性</th>
<th>图片</th>
</tr>
</thead>
<tbody><tr>
<td>字符：42   字符个数：2</td>
<td><img src="/blog/2020/05/22/cv-task1/42.png" alt="标注"></td>
</tr>
<tr>
<td>字符：241   字符个数：3</td>
<td><img src="/blog/2020/05/22/cv-task1/2411.png" alt="标注"></td>
</tr>
<tr>
<td>字符：7358   字符个数：4</td>
<td><img src="/blog/2020/05/22/cv-task1/7358.png" alt="标注"></td>
</tr>
</tbody></table>
<p><strong>因此本次赛题的难点是需要对不定长的字符进行识别，与传统的图像分类识别任务有所不同</strong>。</p>
<h4 id="解题思路探索"><a href="#解题思路探索" class="headerlink" title="解题思路探索"></a>解题思路探索</h4><h5 id="Datawhale给出了三种思路："><a href="#Datawhale给出了三种思路：" class="headerlink" title="Datawhale给出了三种思路："></a>Datawhale给出了三种思路：</h5><h5 id="1、简单入门思路：定长字符识别"><a href="#1、简单入门思路：定长字符识别" class="headerlink" title="1、简单入门思路：定长字符识别"></a>1、简单入门思路：定长字符识别</h5><p>可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符个数为6个。<br>因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。    </p>
<p>经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。可参考Google2014年的论文《Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks》，该论文提出了基于深度卷积神经网络的定长字符分类识别方法。</p>
<h5 id="2、专业字符识别思路：不定长字符识别"><a href="#2、专业字符识别思路：不定长字符识别" class="headerlink" title="2、专业字符识别思路：不定长字符识别"></a>2、专业字符识别思路：不定长字符识别</h5><p><img src="/blog/2020/05/22/cv-task1/%E4%B8%8D%E5%AE%9A%E9%95%BF%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB.png" alt="标注">      </p>
<p>在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。</p>
<p>CRNN采取的架构是CNN+RNN+CTC，cnn提取图像像素特征，rnn提取图像时序特征，而ctc归纳字符间的连接特性。CRNN能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。</p>
<p>此外，<strong>由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。</strong>所有这些属性使得CRNN成为一种基于图像序列识别的极好方法。</p>
<p>在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子。   </p>
<h5 id="3、专业分类思路：检测再识别"><a href="#3、专业分类思路：检测再识别" class="headerlink" title="3、专业分类思路：检测再识别"></a>3、专业分类思路：检测再识别</h5><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。        </p>
<p><img src="/blog/2020/05/22/cv-task1/%E6%A3%80%E6%B5%8B.png" alt="IMG">           </p>
<p>此种思路需要参赛选手构建字符检测模型，对测试集中的字符进行识别。选手可以参考物体检测模型SSD或者YOLO来完成。</p>
<p><strong>SSD算法是一种直接预测目标类别和bounding box的多目标检测算法。</strong>与faster rcnn相比，该算法没有生成 proposal 的过程，这就极大提高了检测速度。针对不同大小的目标检测，传统的做法是先将图像转换成不同大小（图像金字塔），然后分别检测，最后将结果综合起来（NMS）。而SSD算法则利用不同卷积层的 <strong>feature map</strong> 进行综合也能达到同样的效果。算法的主网络结构是VGG16，将最后两个全连接层改成卷积层，并随后增加了4个卷积层来构造网络结构。   </p>
<p><strong>YOLO（You Only Look Once: Unified, Real-Time Object Detection）</strong>，是Joseph Redmon和Ali Farhadi等人于2015年提出的基于单个神经网络的目标检测系统。YOLO是一个可以一次性预测多个Box位置和类别的卷积神经网络，能够实现端到端的目标检测和识别，其最大的优势就是速度快。事实上，目标检测的本质就是回归，因此一个实现回归功能的CNN并不需要复杂的设计过程。YOLO没有选择滑动窗口（silding window）或提取proposal的方式训练网络，而是直接选用整图训练模型。这样做的好处在于可以更好的区分目标和背景区域，相比之下，采用proposal训练方式的Fast-R-CNN常常把背景区域误检为特定目标。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>综上所示，本次赛题虽然是一个简单的字符识别问题，但有多种解法可以使用到计算机视觉领域中的各个模型，是非常适合CV入门学习的。解题思路中由浅至深分析了一个分类识别的问题，也是很有参考价值的，也算是给我们这样的初学者指出了一条不断深入和从不同角度分析解决问题的路子。</p>
<h5 id="鸣谢与参考"><a href="#鸣谢与参考" class="headerlink" title="鸣谢与参考"></a>鸣谢与参考</h5><p>References</p>
<ol>
<li><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/Datawhale%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8CV%20-%20Task%2001%20%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3%20.md" target="_blank" rel="noopener">Datawhale 零基础入门CV赛事-Task1 赛题理解</a></li>
<li><a href="https://blog.csdn.net/wsp_1138886114/article/details/82555728?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">CRNN-基于序列的（端到端）图像文本识别</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1064906" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector 深度学习笔记之SSD物体检测模型</a></li>
<li><a href="https://blog.csdn.net/guoyunfei20/article/details/78744753" target="_blank" rel="noopener">YOLO</a></li>
<li><a href="https://www.cnblogs.com/fariver/p/7446921.html" target="_blank" rel="noopener">YOLO原理</a></li>
<li><a href="https://arxiv.org/pdf/1312.6082.pdf" target="_blank" rel="noopener">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</a></li>
</ol>
]]></content>
      <categories>
        <category>入门</category>
        <category>天池赛题</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>零基础入门CV之数据读取与数据扩增</title>
    <url>//blog/2020/05/23/cv-task2.html</url>
    <content><![CDATA[<h3 id="图像读取"><a href="#图像读取" class="headerlink" title="图像读取"></a>图像读取</h3><p>由于赛题数据是图像数据，赛题的任务是识别图像中的字符。因此我们首先需要完成对数据的读取操作，在Python中有很多库可以完成数据读取的操作，比较常见的有Pillow和OpenCV。     </p>
<h4 id="Pillow"><a href="#Pillow" class="headerlink" title="Pillow"></a>Pillow</h4><p><a href="https://pillow.readthedocs.io/en/stable/" target="_blank" rel="noopener">官方文档</a></p>
<p>Pillow是Python图像处理函式库(PIL）的一个分支。Pillow提供了常见的图像读取和处理的操作，而且可以与ipython notebook无缝集成，是应用比较广泛的库。 </p>
<p>可以简单看下这个库常用的模块：</p>
<p><img src="https://img-blog.csdnimg.cn/20190303145700384.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbnRpYW43Mg==,size_16,color_FFFFFF,t_70" alt></p>
<p> <strong>接着我们来演示一下简单其最基本的几个操作</strong>：               <img src="/blog/2020/05/23/cv-task2/2_1.png" alt="原图"></p>
<p><img src="/blog/2020/05/23/cv-task2/2_2.png" alt="rotate"></p>
<p><img src="/blog/2020/05/23/cv-task2/2_3.png" alt="thumbnail"></p>
<p>Pillow还有很多图像操作，是图像处理的必备库。</p>
<h4 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h4><p>OpenCV是一个跨平台的计算机视觉库，最早由Intel开源得来。OpenCV发展的非常早，拥有众多的计算机视觉、数字图像处理和机器视觉等功能。<strong>OpenCV在功能上比Pillow更加强大很多，学习成本也高很多。</strong></p>
<p>我们可以看看OpenCV的学习思维导图：</p>
<p> <img src="https://img-blog.csdn.net/20160812160354901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt></p>
<p><strong>同样，下面我们演示一些基本的操作：</strong></p>
<p><img src="/blog/2020/05/23/cv-task2/2_4.png" alt></p>
<p><img src="/blog/2020/05/23/cv-task2/2_5.png" alt="gray"></p>
<p><img src="/blog/2020/05/23/cv-task2/2_6.png" alt="canny"></p>
<p>OpenCV包含了众多的图像处理的功能，OpenCV包含了你能想得到的只要与图像相关的操作。此外OpenCV还内置了很多的图像特征处理算法，如关键点检测、边缘检测和直线检测等。<br>OpenCV官网：<a href="https://opencv.org/" target="_blank" rel="noopener">https://opencv.org/</a><br>OpenCV Github：<a href="https://github.com/opencv/opencv" target="_blank" rel="noopener">https://github.com/opencv/opencv</a><br>OpenCV 扩展算法库：<a href="https://github.com/opencv/opencv_contrib" target="_blank" rel="noopener">https://github.com/opencv/opencv_contrib</a></p>
<h3 id="数据扩增方法"><a href="#数据扩增方法" class="headerlink" title="数据扩增方法"></a>数据扩增方法</h3><p>在上一小节中给大家初步介绍了Pillow和OpenCV的使用，现在回到赛题街道字符识别任务中。在赛题中我们需要对的图像进行字符识别，因此需要我们完成的数据的读取操作，同时也需要完成数据扩增（Data Augmentation）操作。     </p>
<h4 id="数据扩增介绍"><a href="#数据扩增介绍" class="headerlink" title="数据扩增介绍"></a>数据扩增介绍</h4><p><strong>在深度学习中数据扩增方法非常重要，数据扩增可以增加训练集的样本，同时也可以有效缓解模型过拟合的情况，也可以给模型带来的更强的泛化能力。</strong></p>
<p><img src="/blog/2020/05/23/cv-task2/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E.png" alt="IMG"></p>
<ul>
<li><h4 id="数据扩增为什么有用？"><a href="#数据扩增为什么有用？" class="headerlink" title="数据扩增为什么有用？"></a>数据扩增为什么有用？</h4>在深度学习模型的训练过程中，数据扩增是必不可少的环节。现有深度学习的参数非常多，一般的模型可训练的参数量基本上都是万到百万级别，而训练集样本的数量很难有这么多。<br>其次数据扩增可以扩展样本空间，假设现在的分类模型需要对汽车进行分类，左边的是汽车A，右边为汽车B。如果不使用任何数据扩增方法，深度学习模型会从汽车车头的角度来进行判别，而不是汽车具体的区别。     </li>
</ul>
<p><img src="/blog/2020/05/23/cv-task2/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9Ecar.png" alt="IMG">   </p>
<ul>
<li><h4 id="有哪些数据扩增方法？"><a href="#有哪些数据扩增方法？" class="headerlink" title="有哪些数据扩增方法？"></a>有哪些数据扩增方法？</h4>数据扩增方法有很多：从颜色空间、尺度空间到样本空间，同时根据不同任务数据扩增都有相应的区别。        <strong>对于图像分类，数据扩增一般不会改变标签；对于物体检测，数据扩增会改变物体坐标位置；对于图像分割，数据扩增会改变像素标签。</strong>      <h4 id="常见的数据扩增方法"><a href="#常见的数据扩增方法" class="headerlink" title="常见的数据扩增方法"></a>常见的数据扩增方法</h4>在常见的数据扩增方法中，一般会从图像颜色、尺寸、形态、空间和像素等角度进行变换。当然不同的数据扩增方法可以自由进行组合，得到更加丰富的数据扩增方法。         </li>
</ul>
<p>以torchvision为例，常见的数据扩增方法包括：</p>
<table>
<thead>
<tr>
<th align="center">扩增方法</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="center">transforms.CenterCrop</td>
<td align="center">对图片中心进行裁剪</td>
</tr>
<tr>
<td align="center">transforms.ColorJitter</td>
<td align="center">调整图像颜色的对比度、饱和度和零度</td>
</tr>
<tr>
<td align="center">transforms.FiveCrop</td>
<td align="center">对图像四个角和中心进行裁剪得到五分图像</td>
</tr>
<tr>
<td align="center">transforms.Grayscale</td>
<td align="center">对图像进行灰度变换</td>
</tr>
<tr>
<td align="center">transforms.Pad</td>
<td align="center">使用固定值进行像素填充</td>
</tr>
<tr>
<td align="center">transforms.RandomAffine</td>
<td align="center">随机仿射变换</td>
</tr>
<tr>
<td align="center">transforms.RandomCrop</td>
<td align="center">随机区域裁剪</td>
</tr>
<tr>
<td align="center">transforms.RandomHorizontalFlip</td>
<td align="center">随机水平翻转</td>
</tr>
<tr>
<td align="center">transforms.RandomRotation</td>
<td align="center">随机旋转</td>
</tr>
<tr>
<td align="center">transforms.RandomVerticalFlip</td>
<td align="center">随机垂直翻转</td>
</tr>
</tbody></table>
<p> <img src="/blog/2020/05/23/cv-task2/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E%E7%A4%BA%E4%BE%8B.png" alt="IMG">    </p>
<p>在本次赛题中，赛题任务是需要对图像中的字符进行识别，因此对于字符图片并不能进行翻转操作。比如字符6经过水平翻转就变成了字符9，会改变字符原本的含义。    </p>
<h4 id="常用的数据扩增库"><a href="#常用的数据扩增库" class="headerlink" title="常用的数据扩增库"></a>常用的数据扩增库</h4><ul>
<li><h4 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a><a href="https://github.com/pytorch/vision" target="_blank" rel="noopener">torchvision</a></h4><p>pytorch官方提供的数据扩增库，提供了基本的数据数据扩增方法，可以无缝与torch进行集成；但数据扩增方法种类较少，且速度中等；下面给出的例子就是用这种方法。       </p>
</li>
<li><h4 id="imgaug"><a href="#imgaug" class="headerlink" title="imgaug"></a><a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">imgaug</a></h4><p>imgaug是常用的第三方数据扩增库，提供了多样的数据扩增方法，且组合起来非常方便，速度较快；      </p>
</li>
<li><h4 id="albumentations"><a href="#albumentations" class="headerlink" title="albumentations"></a><a href="https://albumentations.readthedocs.io" target="_blank" rel="noopener">albumentations</a></h4><p>是常用的第三方数据扩增库，提供了多样的数据扩增方法，对图像分类、语义分割、物体检测和关键点检测都支持，速度较快。      </p>
</li>
</ul>
<h2 id="Pytorch读取数据"><a href="#Pytorch读取数据" class="headerlink" title="Pytorch读取数据"></a>Pytorch读取数据</h2><p>由于本次赛题我们使用Pytorch框架讲解具体的解决方案，接下来将是解决赛题的第一步 —— 使用Pytorch读取赛题数据。<br><strong>在Pytorch中数据是通过Dataset进行封装，并通过DataLoder进行并行读取。</strong></p>
<p>我们先来看一下<strong>Dataset</strong>这个类：</p>
<ol>
<li><strong>作用: 创建数据集，有<code>__getitem__</code>(self, index)函数来根据索引序号获取图片和标签，有<code>__len__</code>(self)函数来获取数据集的长度。</strong></li>
<li><strong>其他的数据集类必须是torch.utils.data.Dataset的子类,比如说torchvision.ImageFolder</strong></li>
</ol>
<p>所以我们只需要重载一下数据读取的逻辑就可以完成数据的读取。      </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sys, glob, shutil, json</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVHNDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_path, img_label, transform=None)</span>:</span></span><br><span class="line">        self.img_path = img_path</span><br><span class="line">        self.img_label = img_label </span><br><span class="line">        <span class="keyword">if</span> transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.transform = transform</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.transform = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img = Image.open(self.img_path[index]).convert(<span class="string">'RGB'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 原始SVHN中类别10为数字0</span></span><br><span class="line">        lbl = np.array(self.img_label[index], dtype=np.int)</span><br><span class="line">        lbl = list(lbl)  + (<span class="number">5</span> - len(lbl)) * [<span class="number">10</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> img, torch.from_numpy(np.array(lbl[:<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.img_path)</span><br><span class="line"></span><br><span class="line">train_path = glob.glob(<span class="string">'../input/train/*.png'</span>)</span><br><span class="line">train_path.sort()</span><br><span class="line">train_json = json.load(open(<span class="string">'../input/train.json'</span>))</span><br><span class="line">train_label = [train_json[x][<span class="string">'label'</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_json]</span><br><span class="line"></span><br><span class="line">data = SVHNDataset(train_path, train_label,</span><br><span class="line">          transforms.Compose([</span><br><span class="line">              <span class="comment"># 缩放到固定尺寸</span></span><br><span class="line">              transforms.Resize((<span class="number">64</span>, <span class="number">128</span>)),</span><br><span class="line"></span><br><span class="line">              <span class="comment"># 随机颜色变换</span></span><br><span class="line">              transforms.ColorJitter(<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),</span><br><span class="line"></span><br><span class="line">              <span class="comment"># 加入随机旋转</span></span><br><span class="line">              transforms.RandomRotation(<span class="number">5</span>),</span><br><span class="line"></span><br><span class="line">              <span class="comment"># 将图片转换为pytorch 的tesntor</span></span><br><span class="line">              <span class="comment"># transforms.ToTensor(),</span></span><br><span class="line"></span><br><span class="line">              <span class="comment"># 对图像像素进行归一化</span></span><br><span class="line">              <span class="comment"># transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])</span></span><br><span class="line">            ]))</span><br></pre></td></tr></table></figure>
<p>通过上述代码，可以将赛题的图像数据和对应标签进行读取，在读取过程中的进行数据扩增，效果如下所示：</p>
<p>​       </p>
<table>
<thead>
<tr>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td><img src="/blog/2020/05/23/cv-task2/23.png" alt="IMG"></td>
<td><img src="/blog/2020/05/23/cv-task2/23_1.png" alt="IMG"></td>
<td><img src="/blog/2020/05/23/cv-task2/23_2.png" alt="IMG"></td>
</tr>
<tr>
<td><img src="/blog/2020/05/23/cv-task2/144_1.png" alt="IMG"></td>
<td><img src="/blog/2020/05/23/cv-task2/144_2.png" alt="IMG"></td>
<td><img src="/blog/2020/05/23/cv-task2/144_3.png" alt="IMG"></td>
</tr>
</tbody></table>
<p>接下来我们将在定义好的Dataset基础上构建DataLoder，你可以会问有了Dataset为什么还要有DataLoder？其实这两个是两个不同的概念，是为了实现不同的功能。                 </p>
<ul>
<li><strong>Dataset：对数据集的封装，提供索引方式的对数据样本进行读取</strong>      </li>
<li><strong>DataLoder：对Dataset进行封装，提供批量读取的迭代读取</strong>    </li>
</ul>
<p>其中，<strong>DataLoder</strong>的参数说明如下：</p>
<ul>
<li><code>dataset</code> 传入的数据集；</li>
<li><code>batch_size</code>每个batch有多少个样本；</li>
<li><code>shuffle</code>是否打乱数据；</li>
<li><code>num_workers</code>有几个进程来处理data loading；win下一般不能多进程，只能设置为0；</li>
<li><code>collate_fn</code>将一个list的sample组成一个mini-batch的函数；</li>
</ul>
<p>其主要逻辑处理过程是先通过Dataset类里面的<code>__getitem__</code>函数获取单个的数据，然后组合成batch，再使用<code>collate_fn</code>所指定的函数对这个batch做一些操作，比如padding之类的。</p>
<p> 加入DataLoder后，数据读取代码改为如下：      </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sys, glob, shutil, json</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVHNDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_path, img_label, transform=None)</span>:</span></span><br><span class="line">        self.img_path = img_path</span><br><span class="line">        self.img_label = img_label </span><br><span class="line">        <span class="keyword">if</span> transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.transform = transform</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.transform = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img = Image.open(self.img_path[index]).convert(<span class="string">'RGB'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 原始SVHN中类别10为数字0</span></span><br><span class="line">        lbl = np.array(self.img_label[index], dtype=np.int)</span><br><span class="line">        lbl = list(lbl)  + (<span class="number">5</span> - len(lbl)) * [<span class="number">10</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> img, torch.from_numpy(np.array(lbl[:<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.img_path)</span><br><span class="line"></span><br><span class="line">train_path = glob.glob(<span class="string">'../input/train/*.png'</span>)</span><br><span class="line">train_path.sort()</span><br><span class="line">train_json = json.load(open(<span class="string">'../input/train.json'</span>))</span><br><span class="line">train_label = [train_json[x][<span class="string">'label'</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_json]</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        SVHNDataset(train_path, train_label,</span><br><span class="line">                   transforms.Compose([</span><br><span class="line">                       transforms.Resize((<span class="number">64</span>, <span class="number">128</span>)),</span><br><span class="line">                       transforms.ColorJitter(<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.2</span>),</span><br><span class="line">                       transforms.RandomRotation(<span class="number">5</span>),</span><br><span class="line">                       transforms.ToTensor(),</span><br><span class="line">                       transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">            ])), </span><br><span class="line">    batch_size=<span class="number">10</span>, <span class="comment"># 每批样本个数</span></span><br><span class="line">    shuffle=<span class="literal">False</span>, <span class="comment"># 是否打乱顺序</span></span><br><span class="line">    num_workers=<span class="number">10</span>, <span class="comment"># 读取的线程个数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>在加入DataLoder后，数据按照批次获取，每批次调用Dataset读取单个样本进行拼接。此时data的格式为：<br>                <code>torch.Size([10, 3, 64, 128]), torch.Size([10, 6])</code><br>前者为图像文件，为<code>batchsize</code>  *  <code>chanel</code>  *  <code>height</code>  *  <code>width</code>次序；后者为字符标签。      </p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章对数据读取进行了详细的讲解，并介绍了常见的数据扩增方法和使用，最后使用Pytorch框架对本次赛题的数据进行读取。 </p>
<h5 id="鸣谢和参考"><a href="#鸣谢和参考" class="headerlink" title="鸣谢和参考"></a>鸣谢和参考</h5><p>References</p>
<ol>
<li><a href="https://blog.csdn.net/wfy2695766757/article/details/81193370" target="_blank" rel="noopener">Python的Pillow库进行图像文件处理</a></li>
<li><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/Datawhale%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8CV%20-%20Task%2002%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E.md" target="_blank" rel="noopener">Datawhale 零基础入门CV赛事-Task2 数据读取与数据扩增</a></li>
<li><a href="https://blog.csdn.net/steventian72/article/details/88088277?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-2" target="_blank" rel="noopener">Python图像处理库PIL的知识点思维导图</a></li>
</ol>
<p>​     </p>
]]></content>
      <categories>
        <category>入门</category>
        <category>天池赛题</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>零基础入门CV之CNN初探</title>
    <url>//blog/2020/05/26/cv-task3.html</url>
    <content><![CDATA[<h2 id="CNN-简介"><a href="#CNN-简介" class="headerlink" title="CNN 简介"></a>CNN 简介</h2><h5 id="1、CNN的组成"><a href="#1、CNN的组成" class="headerlink" title="1、CNN的组成"></a>1、CNN的组成</h5><p>CNN由纽约大学的Yann LeCun于1998年提出。本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。CNN是一类特殊的人工神经网络，是深度学习中重要的一个分支。CNN在很多领域都表现优异，精度和速度比传统计算学习算法高很多。特别是在计算机视觉领域，CNN是解决图像分类、图像检索、物体检测和语义分割的主流模型。CNN通常由卷积（convolution）、池化（pooling）、非线性激活函数（non-linear activation function）和全连接层（fully connected layer）构成。</p>
<p>其中conv层和pooling层简介如下：</p>
<ol>
<li>conv层的理论依据主要是生物学上的感受野概念，通俗点讲就是权值共享，这样可以大大减少神经网络需要训练的参数；卷积核是CNN的重要组成，常见做法是从原始图像中采样，基于无监督学习算法进行训练得到，这一步就是DNN中的预训练。</li>
<li>pooling层，其实就是对图像进行子采样，利用图像局部相关性的原理，在减少数据量的同时又保留有用信息。常见的pooling有max跟mean，就是对每一个采样区域做max运算或者mean运算，既可以减少参数，又可以保留信息，与此同时，还引入平移不变性等图像性质。</li>
</ol>
<h5 id="2、CNN模型的典型结构"><a href="#2、CNN模型的典型结构" class="headerlink" title="2、CNN模型的典型结构"></a>2、CNN模型的典型结构</h5><ol>
<li>LeNet：最早用于数字识别的CNN；</li>
<li>AlexNet：2012年 ILSVRC比赛中远超第二名的CNN，用多层小卷积层替换掉单层大卷积层；</li>
<li>GoogLeNet：2014年 ILSVRC比赛冠军；</li>
<li>VGGNet：2014年表现略差于GoogLeNet，但是在其他图像转换领域表现较好，如目标检测领域；</li>
<li>InceptionV3</li>
<li>ResNet<br><img src="https://upload-images.jianshu.io/upload_images/4070307-d58cba17916ff535.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="comp"></li>
</ol>
<h2 id="用pytorch构建简单的CNN模型"><a href="#用pytorch构建简单的CNN模型" class="headerlink" title="用pytorch构建简单的CNN模型"></a>用pytorch构建简单的CNN模型</h2><h5 id="1、导入所需包"><a href="#1、导入所需包" class="headerlink" title="1、导入所需包"></a>1、导入所需包</h5><p><img src="/blog/2020/05/26/cv-task3/img1.jpg" alt="tu1"></p>
<h5 id="2、构建一个简单的CNN模型"><a href="#2、构建一个简单的CNN模型" class="headerlink" title="2、构建一个简单的CNN模型"></a>2、构建一个简单的CNN模型</h5><p><img src="/blog/2020/05/26/cv-task3/img4.jpg" alt="tu1"><br><img src="/blog/2020/05/26/cv-task3/img3.jpg" alt="tu1"><br>从<code>print</code>结果可以看出，我们构建了一个由两层卷积层、两层基于max的pool层和两个激活函数构成的简单模型。</p>
<h5 id="3、定义损失函数和优化器"><a href="#3、定义损失函数和优化器" class="headerlink" title="3、定义损失函数和优化器"></a>3、定义损失函数和优化器</h5><p><img src="/blog/2020/05/26/cv-task3/img2.jpg" alt="tu1"><br>我们使用交叉验证损失函数和Adam优化器来训练我们的简单模型。</p>
<h5 id="4、训练网络"><a href="#4、训练网络" class="headerlink" title="4、训练网络"></a>4、训练网络</h5><p><img src="/blog/2020/05/26/cv-task3/img5.jpg" alt="tu1"><br><img src="/blog/2020/05/26/cv-task3/img6.jpg" alt="tu1"></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>以上介绍了CNN并用pytorch构建了一个很简单的网络模型，还对该模型进行了必要的分析，以便有一个感性的认识。CNN用处很大，目前了解只是皮毛，还需通过实践来进一步深入的了解和加深实践经验。后续有用到或者有时间再做深入吧。</p>
<h5 id="鸣谢与引用"><a href="#鸣谢与引用" class="headerlink" title="鸣谢与引用"></a>鸣谢与引用</h5><ol>
<li><a href="https://zm.sm-tc.cn/?src=l4uLj4zF0NCdk5CY0ZyMm5HRkZqL0JuNmp6SoJyei5yXmo2gzs%2FQno2LlpyTmtCbmouelpOM0MvIz8%2FOz8rO&uid=2a9b68a44a4de1455fea961a81551e33&hid=884939928472aaaae9ef4cbb2001feec&pos=4&cid=9&time=1590485479485&from=click&restype=1&pagetype=0020004002010402&bu=news_natural&query=CNN%E6%A8%A1%E5%9E%8B&mode=&v=1&force=true&wap=false&province=%E5%B9%BF%E4%B8%9C%E7%9C%81&city=%E5%B9%BF%E5%B7%9E%E5%B8%82&uc_param_str=dnntnwvepffrgibijbprsvdsdichei" target="_blank" rel="noopener">CNN模型</a></li>
<li><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/Datawhale%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8CV%20-%20Task%2003%20%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.md" target="_blank" rel="noopener">Datawhale 零基础入门CV赛事-Task3 字符识别模型</a></li>
</ol>
]]></content>
      <categories>
        <category>入门</category>
        <category>天池赛题</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>零基础入门CV之模型评估与性能度量</title>
    <url>//blog/2020/05/29/cv-task4.html</url>
    <content><![CDATA[<h2 id="模型评估方法"><a href="#模型评估方法" class="headerlink" title="模型评估方法"></a>模型评估方法</h2><p>现实任务中，我们往往有多种学习算法可供选择，甚至对于同一个学习算法，当我们使用不同的参数配置时，也会产生不同的模型。这种情况下，我们选择模型时就得比较和评估，以便我们能选择出泛化性能最好的那个模型。</p>
<p>用学生学习的情况来类比，我们希望学生在一定的课本知识<strong>(训练集)</strong>上学习到某些知识，然后在平时的作业和模拟考<strong>(验证集)</strong>中检查自己学的知识和实际的知识点之间是否有出入，以便最后在期终考试<strong>(测试集、实际应用数据)</strong>有一个比较好的表现——教全面地掌握所要学习的知识。</p>
<p>通过上面这个类比，想必对于大家在机器学习领域常常听说的训练集、验证集和测试集有相对清晰的认识了吧。对我们评估的目标——泛化性能较强这个目标想必也有所理解了。</p>
<h4 id="训练集、验证集和测试集"><a href="#训练集、验证集和测试集" class="headerlink" title="训练集、验证集和测试集"></a>训练集、验证集和测试集</h4><p>我们先来看看为什么要分这三种数据集。在机器学习的过程中，我们的目标是找到那个泛化性能最好的模型，因此我们有两方面的参数需要确定：1、模型里函数的参数，也就是我们常说的权重矩阵w和修正b；这类参数一般在训练过程中通过各种最优化算法求得；2、模型参数，比如多项式回归的次数、规则化参数λ等；这些参数也成为超参数。很明显，为了获得这些参数，我们需要在数据集上训练模型，这就需要一个训练集。为了评估训练出来的模型，以便选择一个效果最好的模型，我们需要一个验证集；为了用上一步选出的最优模型来进行泛化性能评估，我们需要一个测试集。所以，这三个集合的用途分别是：</p>
<ul>
<li><h4 id="训练集（Train-Set）：用于训练模型和调整模型参数-如w、b-；"><a href="#训练集（Train-Set）：用于训练模型和调整模型参数-如w、b-；" class="headerlink" title="训练集（Train Set）：用于训练模型和调整模型参数(如w、b)；"></a>训练集（Train Set）：用于训练模型和调整模型参数(如w、b)；</h4></li>
<li><h4 id="验证集（Validation-Set）：用来验证模型精度和确定模型超参数，以便选出最优模型；"><a href="#验证集（Validation-Set）：用来验证模型精度和确定模型超参数，以便选出最优模型；" class="headerlink" title="验证集（Validation Set）：用来验证模型精度和确定模型超参数，以便选出最优模型；"></a>验证集（Validation Set）：用来验证模型精度和确定模型超参数，以便选出最优模型；</h4></li>
<li><h4 id="测试集（Test-Set）：仅用于对最优模型进行性能评估，验证模型的泛化能力。"><a href="#测试集（Test-Set）：仅用于对最优模型进行性能评估，验证模型的泛化能力。" class="headerlink" title="测试集（Test Set）：仅用于对最优模型进行性能评估，验证模型的泛化能力。"></a>测试集（Test Set）：仅用于对最优模型进行性能评估，验证模型的泛化能力。</h4></li>
</ul>
<p>一般来说，我们拿到的都只有一个数据集D，那如何对D进行适当的处理，以便从中产生出训练集S、验证集V和测试集T呢？常用的方法有一下三种：</p>
<h5 id="1、留出法（Hold-Out）"><a href="#1、留出法（Hold-Out）" class="headerlink" title="1、留出法（Hold-Out）"></a>1、留出法（Hold-Out）</h5><p>直接将数据集D划分成两个互斥的部分——训练集S和验证集T。一般来说是把其中的2/3~4/5划为训练集，其余为验证集；与此同时，为了保证划分的集合分布与原集合分布一致，通常使用“分层采样”的方式划分。这种划分方式的优点是最为直接简单，缺点是只得到了一份验证集，有可能导致模型在验证集上过拟合。因此，留出法应用场景一般是数据量比较大的情况。</p>
<h5 id="2、交叉验证法（Cross-Validation，CV）"><a href="#2、交叉验证法（Cross-Validation，CV）" class="headerlink" title="2、交叉验证法（Cross Validation，CV）"></a>2、交叉验证法（Cross Validation，CV）</h5><p>将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。也就是K折交叉验证。如果K=1，则叫留一法。这种划分方式是所有的训练集都是验证集，最终模型验证精度是K份平均得到。优点是验证集精度比较可靠，训练K次可以得到K个有多样性差异的模型；C缺点是需要训练K次，导致复杂度过大，不适合数据量很大的情况。</p>
<h5 id="3、自助采样法（BootStrap）"><a href="#3、自助采样法（BootStrap）" class="headerlink" title="3、自助采样法（BootStrap）"></a>3、自助采样法（BootStrap）</h5><p>这种方法我是通过吃自助餐理解的，就是一种数据自助取用的方式。通过有放回的采样方式得到新的训练集和验证集，每次的训练集和验证集都是有区别的。通过概率计算可以知道，大概有2/3左右的数据会被采样到训练集，而有1/3是没有在训练集出现的。这种划分方式一般适用于数据量较小，难以有效划分的情况。此外，自助法能从初始数据集中产生出多个不同的训练集，对于集成学习来说是有利的。缺点是采样过程中一定程度上改变了初始数据集的分布，也就是引入了估计偏差。因此，一般来说用前面两种方法比较多。</p>
<h2 id="模型性能度量"><a href="#模型性能度量" class="headerlink" title="模型性能度量"></a>模型性能度量</h2><p>对模型的泛化性能进行评估，不仅仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure)。</p>
<p>一般来说，性能度量是反映任务需求的。就是说，我们要实事求是，不同类型的任务使用不同的性能度量，好的模型不仅取决于算法和数据，还取决于任务需求。</p>
<h4 id="回归任务的评估"><a href="#回归任务的评估" class="headerlink" title="回归任务的评估"></a>回归任务的评估</h4><p><img src="/blog/2020/05/29/cv-task4/tu1.png" alt></p>
<h4 id="分类任务的评估"><a href="#分类任务的评估" class="headerlink" title="分类任务的评估"></a>分类任务的评估</h4><p><strong>使用混淆矩阵评估模型</strong>:</p>
<p><img src="/blog/2020/05/29/cv-task4/tu4.png" alt></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>本文简要梳理了机器学习中常见的三种数据集类别：训练集、验证集和测试集；并对常见的三种数据集划分方式做了介绍。最后提了一下模型的性能度量，了解还不够深入，留以后再做补充。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li>《机器学习》. 周志华</li>
<li><a href="http://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation" target="_blank" rel="noopener">How to choose a predictive model after k-fold cross-validation</a></li>
<li><a href="https://www.jianshu.com/p/21cb3ad928c6" target="_blank" rel="noopener">模型性能度量方法</a></li>
</ol>
]]></content>
      <categories>
        <category>入门</category>
        <category>天池赛题</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>零基础入门CV之集成学习</title>
    <url>//blog/2020/06/02/cv-task5.html</url>
    <content><![CDATA[<h2 id="集成学习是什么"><a href="#集成学习是什么" class="headerlink" title="集成学习是什么"></a>集成学习是什么</h2><h2 id="一、集成学习是什么"><a href="#一、集成学习是什么" class="headerlink" title="一、集成学习是什么"></a>一、集成学习是什么</h2><p>“团结就是力量”、“综合就是全面”，这是机器学习领域中”集成学习“的基本思想。大多数数据竞赛排名靠前的解决方案所采用的集成方法，都建立在一个假设：组合多个模型以期得到一个更全面的模型，从而得到一个泛化性能更好的模型。</p>
<p>集成学习一般有Bagging、Boosting和Stacking三种类别：</p>
<ul>
<li><strong>Bagging</strong>：利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出综合得到。具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。</li>
<li><strong>Boosting</strong>：一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。</li>
<li><strong>Stacking</strong>：指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。</li>
</ul>
<h2 id="二、集成学习的应用场景"><a href="#二、集成学习的应用场景" class="headerlink" title="二、集成学习的应用场景"></a>二、集成学习的应用场景</h2><p>集成学习的主要应用场景有以下几个方面：</p>
<ol>
<li><strong>模型选择</strong>：相比于只选择一个模型，集成学习对模型集合中所有模型的输出按照某种方法进行组合，例如简单的取平均，可以降低模型选择不当的风险。这个要求能达到适当的多样性，这样才能通过一定的策略优势互补地减少总体误差。</li>
<li><strong>分而治之</strong>：集成学习系统遵循一种分而治之的方法，将数据空间划分为更小、更易于学习的分区，其中每个模型只学习其中一个更简单的分区。然后通过不同方法来适当组合模型的输出来近似复杂决策边界。这可以解决难以用一个模型来处理的机器学习问题。</li>
<li><strong>数据融合</strong>：在许多需要自动决策的应用程序中，接收来自不同来源的数据并提供补充信息是很常见的。这种信息的适当组合被称为<strong>数据或信息融合</strong>，与仅根据任何单个数据源进行决策相比，可以提高分类决策的准确性。</li>
<li><strong>置信度估计</strong>：考虑对一个分类问题训练一个分类器集合，如果集合中绝大多数分类器决策一致，那么就认为整体决策的结果具有较高置信度；反之则具有较低置信度。</li>
<li><strong>数据扩增</strong>：如果训练数据量太小，那么使用bootstrapping技术可以从总体数据集中有放回的随机采样获得多个样本集，每个样本集作为训练集训练一个模型，最后再以一定策略组合。</li>
</ol>
<h2 id="三、集成学习的结合策略"><a href="#三、集成学习的结合策略" class="headerlink" title="三、集成学习的结合策略"></a>三、集成学习的结合策略</h2><p>模型集合的组合方法有很多，我们这里主要介绍在实践中最常用的四种：线性组合、乘积组合、投票组合和学习组合。</p>
<ol>
<li><strong>线性组合</strong>：线性组合用于输出实数的模型，因此适用于回归模型结果的集成，或分类模型集成中估计类别的概率。</li>
<li><strong>乘积组合</strong>：在各模型的类别条件概率估计相互独立的假设下，乘积组合在理论上是最好的组合策略。</li>
<li><strong>投票组合</strong>：每个分类器对某个类别投票，获得多数票的类别作为集成模型整体的输出。包括相对多数投票法、绝对多数投票法和加权投票法。</li>
<li><strong>学习组合</strong>：通过机器学习的方式来组合，代表是Stacking。我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.jianshu.com/p/3e8c44314be5" target="_blank" rel="noopener">集成学习(Ensemble learning)</a></li>
<li><a href="https://www.cnblogs.com/wxquare/p/5440664.html" target="_blank" rel="noopener">集成学习方法</a></li>
</ol>
]]></content>
      <categories>
        <category>入门</category>
        <category>天池赛题</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of CS224N Lecture1 —— Introduction and word vectors</title>
    <url>//blog/2020/06/24/notes-of-CS224n-part1.html</url>
    <content><![CDATA[<h1 id="Note-of-CS224N-Lecture1-Introduction-and-word-vectors"><a href="#Note-of-CS224N-Lecture1-Introduction-and-word-vectors" class="headerlink" title="Note of CS224N Lecture1: Introduction and word vectors"></a>Note of CS224N Lecture1: Introduction and word vectors</h1><blockquote>
<p>School:  Stanford</p>
<p>Teacher: Prof. Christopher Manning</p>
<p>Library: Pytorch</p>
<p>Course: <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html" target="_blank" rel="noopener">CS224n</a></p>
</blockquote>
<h2 id="1-What’s-NLP"><a href="#1-What’s-NLP" class="headerlink" title="1. What’s NLP?"></a>1. What’s NLP?</h2><h4 id="a-Definition"><a href="#a-Definition" class="headerlink" title="a. Definition"></a>a. Definition</h4><p>Natural Language Processing(NLP for short),  is broadly defined as the automatic manipulation of natural language which refers to the way we humans communicate with each other, like speech and text, by software. </p>
<h4 id="b-Basic-tasks"><a href="#b-Basic-tasks" class="headerlink" title="b. Basic tasks"></a>b. Basic tasks</h4><p>The study of NLP has been around for more than half a century, and the target of NLP is helping computers understand, interpret and manipulate human language, which isn’t really solved. </p>
<p>NLP includes many different techniques for interpreting human language, ranging from statistical and machine learning methods to rules-based and algorithmic approaches. We need a broad array of approaches because the text- and voice-based data varies widely, as do the practical applications. </p>
<p>Basic NLP tasks include tokenization and parsing, lemmatization/stemming, part-of-speech tagging, language detection and identification of semantic relationships. If you ever diagramed sentences in grade school, you’ve done these tasks manually before. </p>
<p>In general terms, NLP tasks break down language into shorter, elemental pieces, try to understand relationships between the pieces and explore how the pieces work together to create meaning.</p>
<h5 id="1-Content-categorization"><a href="#1-Content-categorization" class="headerlink" title="1. Content categorization"></a>1. Content categorization</h5><h5 id="2-Topic-discovery-and-modeling"><a href="#2-Topic-discovery-and-modeling" class="headerlink" title="2. Topic discovery and modeling"></a>2. Topic discovery and modeling</h5><h5 id="3-Contextual-extraction"><a href="#3-Contextual-extraction" class="headerlink" title="3. Contextual extraction"></a>3. Contextual extraction</h5><h5 id="4-Sentiment-analysis"><a href="#4-Sentiment-analysis" class="headerlink" title="4. Sentiment analysis"></a>4. Sentiment analysis</h5><h5 id="5-Speech-to-text-and-text-to-speech-conversion"><a href="#5-Speech-to-text-and-text-to-speech-conversion" class="headerlink" title="5. Speech-to-text and text-to-speech conversion"></a>5. Speech-to-text and text-to-speech conversion</h5><h5 id="6-Document-summarization"><a href="#6-Document-summarization" class="headerlink" title="6. Document summarization"></a>6. Document summarization</h5><h5 id="7-Machine-translation"><a href="#7-Machine-translation" class="headerlink" title="7. Machine translation"></a>7. Machine translation</h5><h2 id="2-What’s-word-vectors"><a href="#2-What’s-word-vectors" class="headerlink" title="2. What’s word vectors?"></a>2. What’s word vectors?</h2><p><img src="/blog/2020/06/24/notes-of-CS224n-part1/word_vectors.png" alt></p>
<p>Obviously, the first step of all NLP tasks is how we represent words as input to any our models. Word vectors which are the texts converted into numbers and there may be different numerical representations of the same text, give a way to this problem. </p>
<p>A Word vector format generally tries to map a word using a dictionary to a vector. Let us break this phrase down into finer details to have a clear view.</p>
<p>Take a look at this example - “The banking system is a basic system”; A dictionary may be the list of all unique words in the sentence. So, a dictionary may look like – [‘The’,’banking’,’system’,’is’,’a’,’basic’]. A vector representation of a word may be a one-hot encoded vector, like a vector show in above picture.</p>
<h5 id="Different-types-of-Word-Vectors"><a href="#Different-types-of-Word-Vectors" class="headerlink" title="Different types of Word Vectors"></a>Different types of Word Vectors</h5><p>The different types of word vectors can be broadly classified into two categories-</p>
<ol>
<li>Frequency based</li>
<li>Prediction based</li>
</ol>
<h5 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h5><p><img src="/blog/2020/06/24/notes-of-CS224n-part1/word2vec.png" alt></p>
<h2 id="3-What’s-next"><a href="#3-What’s-next" class="headerlink" title="3. What’s next?"></a>3. What’s next?</h2><p>I joined in a NLP learning activity held by an open-source organization <a href="https://datawhale.club/" target="_blank" rel="noopener">Datawhale</a> recently. We are gonna learning the awesome course <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html" target="_blank" rel="noopener">CS224n</a> by Stanford in this activity. </p>
<p>I will keep updating my progress and my learning note in the upcoming posts on this blog. As a beginner of this field, I have a lot to learn. I may just follow the plan of this activity or this course to have a big picture of NLP. So, my next post may be the exploring of Word2vec.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p><a href="https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html#howitworks" target="_blank" rel="noopener">NLP: What it is and why it matters</a></p>
</li>
<li><p><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>入门</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploring of Word2vec</title>
    <url>//blog/2020/06/26/exploring-of-word2vec.html</url>
    <content><![CDATA[<blockquote>
<p>Word2vec is a deep learning technique that feeds massive amounts of text into a shallow neural net which can then be used to solve a variety of NLP and ML problems. Usually, Word2vec Explorer uses <a href="https://github.com/piskvorky/gensim" target="_blank" rel="noopener">Gensim</a> to list and compare vectors. </p>
</blockquote>
<h2 id="1-recommented-refs-of-Word2vec"><a href="#1-recommented-refs-of-Word2vec" class="headerlink" title="1. recommented refs of Word2vec"></a>1. recommented refs of Word2vec</h2><ol>
<li>Mikolov’s papers:<ol>
<li><a href="https://arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a> Word2vec, a more powerful language model framework, is proposed and used to generate word vectors.</li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient estimation of word representations in vector space</a> mainly about two tricks of word2vec: hierarchical softmax, negative sampling.</li>
</ol>
</li>
<li>Yoav Goldberg’s paper: <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank" rel="noopener">word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method</a> good for learning more about the formula of negative sampling.</li>
<li>Xin Rong’s paper: <a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a> highly recommented! There are both high-level intuition explanations and detailed derivation processes.</li>
<li>Siwei lai’s paper: <a href="https://arxiv.org/ftp/arxiv/papers/1611/1611.05962.pdf" target="_blank" rel="noopener">Word and Document Embeddings based on Neural Network Approaches</a> </li>
</ol>
<h2 id="2-Word2vec"><a href="#2-Word2vec" class="headerlink" title="2. Word2vec"></a>2. Word2vec</h2><h3 id="2-1-what’s-Word2vec？"><a href="#2-1-what’s-Word2vec？" class="headerlink" title="2.1 what’s Word2vec？"></a>2.1 what’s Word2vec？</h3><p>To perform NLP, we need to find a way to vectorise words so that we can input them into our machine. So we need word embedding, which can convert symbolic words into numerical forms, in other word, embedded into a mathematical space. And Word2vec is a kind of word embedding. </p>
<p>Word2vec only care about the model parameters (specifically the weight of the neural network), and take these parameters as a kind of vectorized representation of the input word. That means we can use Word2vec to generate word vectors.</p>
<h3 id="2-2-Skip-gram-and-CBOW"><a href="#2-2-Skip-gram-and-CBOW" class="headerlink" title="2.2 Skip-gram and CBOW"></a>2.2 Skip-gram and CBOW</h3><ul>
<li><strong>Skip-gram</strong>: a model use a word as input to predict the context around it.</li>
<li><strong>CBOW</strong>: a model use the context around a word as input to predict the word itself.</li>
</ul>
<h4 id="2-2-1-Skip-gram"><a href="#2-2-1-Skip-gram" class="headerlink" title="2.2.1 Skip-gram"></a>2.2.1 Skip-gram</h4><p>we can take a look at the Skip-gram network structure:</p>
<p><img src="/blog/2020/06/26/exploring-of-word2vec/vec.jpg" alt></p>
<p>which x is one-hot encoder which is a vector only containing one one and all other zeros and used to uniquely represent a word; y is the probability of these V words output as a group.</p>
<p>Let’s notice this: <strong>the activation function of the hidden layer is actually linear</strong>, which the key method of Word2vec simplify the previous language model. And we use BP to train this neural network. After training, we got the weights of this net. Because only the weight corresponding to the different position of 1’s is activated, this Vx which is consisted of the weights we got can be used to uniquely represent x.  And the dimension of Vx is generally far smaller than the total number of words V, so Word2vec is essentially a dimensionality reduction operation.</p>
<p>while y has multiple words, the network structure is as follows:</p>
<p><img src="/blog/2020/06/26/exploring-of-word2vec/vet2.jpg" alt></p>
<p>It can be seen as a parallel of a single x-&gt;single y model, and the cost function is the accumulation of a single cost function (after taking log). </p>
<h4 id="2-2-2-CBOW"><a href="#2-2-2-CBOW" class="headerlink" title="2.2.2 CBOW"></a>2.2.2 CBOW</h4><p>Similar to Skip-gram, except Skip-gram predicts the context of a word, and CBOW predicts the word using context.</p>
<p>Network structure is as follow:</p>
<p><img src="/blog/2020/06/26/exploring-of-word2vec/cbow.jpg" alt></p>
<p>It is different from the Skip-gram model in parallel. Here, the input becomes multiple words, so we need to process the input (generally sum and then average) first. The cost function of the output is unchanged. For detail you can read this paper: <a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a>.</p>
<h3 id="2-3-training-tricks"><a href="#2-3-training-tricks" class="headerlink" title="2.3 training tricks"></a>2.3 training tricks</h3><p>Why do we need training tricks? As we just mentioned, Word2vec is essentially a language model, its output node number is V, corresponding to V words, which is essentially a multi-classification problem. But in reality, the number of words is very, very large. It will cause great difficulty in calculation, so we need to use tricks to accelerate the training.</p>
<p>Here are two tricks:</p>
<ul>
<li><p>hierarchical softmax: turn the N classification problem into log(N) secondary classification.</p>
</li>
<li><p>negative sampling: predict a subset of the overall category.</p>
</li>
</ul>
<h2 id="3-References"><a href="#3-References" class="headerlink" title="3. References"></a>3. References</h2><h5 id="1-Natural-Language-Processing1-Word-To-Vectors"><a href="#1-Natural-Language-Processing1-Word-To-Vectors" class="headerlink" title="1. Natural Language Processing1: Word To Vectors"></a>1. <a href="https://billmazengou.github.io/2020/06/23/NLP1-Word-to-Vectors/" target="_blank" rel="noopener">Natural Language Processing1: Word To Vectors</a></h5><h5 id="2-Word2Vec-Explorer"><a href="#2-Word2Vec-Explorer" class="headerlink" title="2. Word2Vec Explorer"></a>2. <a href="https://github.com/dominiek/word2vec-explorer" target="_blank" rel="noopener">Word2Vec Explorer</a></h5><h5 id="3-Distributed-Representations-of-Sentences-and-Documents"><a href="#3-Distributed-Representations-of-Sentences-and-Documents" class="headerlink" title="3. Distributed Representations of Sentences and Documents"></a>3. <a href="https://arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a></h5><h5 id="4-Efficient-estimation-of-word-representations-in-vector-space"><a href="#4-Efficient-estimation-of-word-representations-in-vector-space" class="headerlink" title="4. Efficient estimation of word representations in vector space"></a>4. <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient estimation of word representations in vector space</a></h5><h5 id="5-word2vec-Explained-deriving-Mikolov-et-al-’s-negative-sampling-word-embedding-method"><a href="#5-word2vec-Explained-deriving-Mikolov-et-al-’s-negative-sampling-word-embedding-method" class="headerlink" title="5.  word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method"></a>5.  <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank" rel="noopener">word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method</a></h5><h5 id="6-word2vec-Parameter-Learning-Explained"><a href="#6-word2vec-Parameter-Learning-Explained" class="headerlink" title="6. word2vec Parameter Learning Explained"></a>6. <a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a></h5><h5 id="7-Word-and-Document-Embeddings-based-on-Neural-Network-Approaches"><a href="#7-Word-and-Document-Embeddings-based-on-Neural-Network-Approaches" class="headerlink" title="7.  Word and Document Embeddings based on Neural Network Approaches"></a>7.  <a href="https://arxiv.org/ftp/arxiv/papers/1611/1611.05962.pdf" target="_blank" rel="noopener">Word and Document Embeddings based on Neural Network Approaches</a></h5>]]></content>
      <categories>
        <category>入门</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of CS224n —— GloVe, Evaluation and Training</title>
    <url>//blog/2020/06/26/notes-of-cs224n-part2.html</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>入门</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
